{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, patients, sampling=None, scaled=True):\n",
    "        if sampling not in [None, \"undersampling\", \"oversampling\"]:\n",
    "            raise ValueError(\"Sampling must be one of None, undersampling or oversampling\")\n",
    "        self.segment_files = []\n",
    "        self.labels = []\n",
    "        for patient in patients:\n",
    "            if scaled:\n",
    "                interictal_segment_files = glob(f\"data/segments/scaled/{patient}/interictal/{patient}_interictal_scaled_segment_*.parquet\")\n",
    "                preictal_segment_files= glob(f\"data/segments/scaled/{patient}/preictal/{patient}_preictal_scaled_segment_*.parquet\")\n",
    "            else:\n",
    "                interictal_segment_files = glob(f\"data/segments/raw/{patient}/interictal/{patient}_interictal_segment_*.parquet\")\n",
    "                preictal_segment_files= glob(f\"data/segments/raw/{patient}/preictal/{patient}_preictal_segment_*.parquet\")\n",
    "            if sampling == \"undersampling\":\n",
    "                interictal_segment_files = list(np.random.choice(interictal_segment_files, size=len(preictal_segment_files), replace=False))\n",
    "            elif sampling == \"oversampling\":\n",
    "                preictal_segment_files = list(np.random.choice(preictal_segment_files, size=len(interictal_segment_files), replace=True))\n",
    "            self.segment_files.extend(interictal_segment_files + preictal_segment_files)\n",
    "            self.labels.extend([0.0 for file in interictal_segment_files] + [1.0 for file in preictal_segment_files])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        segment_file = self.segment_files[idx]\n",
    "        segment_df = pd.read_parquet(segment_file).fillna(0)\n",
    "        segment_feature_array = np.concatenate([segment_df.mean(), segment_df.std()])\n",
    "        segment_features = torch.Tensor(segment_feature_array)\n",
    "        label = self.labels[idx]\n",
    "        return segment_features, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(12, 10)\n",
    "        self.linear_2 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear_1(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear_2(output)\n",
    "        output = output.squeeze()\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x).squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, y)\n",
    "        self.log(\"Training Loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x).squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, y)\n",
    "        self.log(\"Validation Loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "        return optimizer         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:4320\n",
      "Testinging samples:480\n"
     ]
    }
   ],
   "source": [
    "train_patients = [\"MSEL_00172\", \"MSEL_00501\", \"MSEL_01097\", \"MSEL_01575\", \"MSEL_01808\", \"MSEL_01838\"]\n",
    "test_patients = [\"MSEL_01842\"]\n",
    "\n",
    "train_data = SegmentDataset(train_patients, sampling=\"undersampling\")\n",
    "test_data = SegmentDataset(test_patients, sampling=\"undersampling\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "print(f\"Training samples:{len(train_data)}\")\n",
    "print(f\"Testinging samples:{len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type   | Params\n",
      "------------------------------------\n",
      "0 | linear_1 | Linear | 130   \n",
      "1 | linear_2 | Linear | 11    \n",
      "2 | relu     | ReLU   | 0     \n",
      "------------------------------------\n",
      "141       Trainable params\n",
      "0         Non-trainable params\n",
      "141       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bailey\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:372: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Bailey\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/38 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bailey\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 38/38 [00:15<00:00,  2.46it/s, loss=0.657, v_num=36]\n"
     ]
    }
   ],
   "source": [
    "model = LogRegClassifier()\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=50, log_every_n_steps=1)\n",
    "trainer.fit(model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38c3778fb65eae4f20aedf3082c4df54eca5849188701ff3fc6acd2b6ef40ace"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
